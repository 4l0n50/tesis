In this section we discuss two different ways to aggregate GS equations. The first is a direct application of the proof of equal commitment opening and is only valid for two-sided linear equations in $\Z_q$, the second is an extension of the results of Jutla and Roy for all other types of linear equations.

\subsection{Aggregating Two-Sided Linear Equations in $\Z_q$}
\label{sec:ts-zq}
We note that proving that $n$ pairs of GS commitments open (pairwise) to the same elements in $\Z_q$ is simply a special case of 
the proof of equal commmitment opening in Sect.~\ref{sec:aggcomms}. Indeed, the concatenation of $n$ GS commitments is just a commitment to a vector of scalars. In particular,  given $\mathsf{crs}_\GS=(\Gamma,\hvecb{u}_1,\hvecb{u}_2,\cvecb{v}_1,\cvecb{v}_2)$,  it is easy to see that  $n$ commitments to $x_i \in \Z_q$, which are of the form:
 $\hvecb{c}_i= x_i \hvecb{u}_1 +r_i \hvecb{u}_2$ for some $r_i \in \Z_q$ (recall that $\iota_1(x_i)= x_i \hvecb{u}_1$), can be written as  
 $$\begin{pmatrix} \hvecb{c}_1 \\ \vdots \\ \hvecb{c}_n \end{pmatrix}=   \begin{pmatrix} \hvecb{u}_1 & \ldots & \hvecb{0}\\ \vdots & \ddots & \vdots \\   \hvecb{0} & \ldots & \hvecb{u}_1  \end{pmatrix} \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}
 + \begin{pmatrix} \hvecb{u}_2 & \ldots & \hvecb{0}\\ \vdots & \ddots & \vdots \\  \hvecb{0} & \ldots & \hvecb{u}_2  \end{pmatrix}
 \begin{pmatrix} r_1 \\ \vdots \\ r_n \end{pmatrix},
 $$
and similarly the concatenation of $n$ commitments $\cvecb{d}_i$, $i \in [\ell]$ can be written as $\cmatr{V}_1\vecb{y}+\cmatr{V}_2 \vecb{s}$, where $\hmatr{V}_i$ is the blockwise concatenation of $n$ copies of $\cvecb{v}_i$.  

In particular, proving that $n$ GS commitments open to the same value can be also seen as the aggregation of the proof  of $n$ GS equations of the form $\varx_\ell -\vary_\ell=0$. The aggregation of any other set of two-sided linear equations in $\Z_q$
easily reduces to this case using the homomorphic properties of GS commitments. Indeed, given $n$ equations of the form:
 $$  \grkb{\alpha}_{\ell}^{\top} \vecb{\vary}+ \vecb{\varx}^{\top}\boldsymbol \beta_{\ell} =t_\ell, \ \ell \in [n],$$
 and the commitments to a satisfying assignment (where the commitments to every coordinate of $\vecb{\varx}$ (resp. $\vecb{\vary}$) are in $\Gr$ (resp. $\Hr$), it is easy to derive a commitment to $\vecb{\varx}^{\top} \grkb{\beta}_{\ell}- t_{\ell}$ in $\Gr$ and a commitment to  $\grkb{\alpha}_{\ell}^{\top} \vecb{\vary}$ in $\Hr$ for all $\ell \in [n]$. Obviously, the equations are satisfied if for each $\ell$, these commitments open to the same value. 

We insist that two-sided linear equations in $\Z_q$ are essential to prove quadratic statements in asymmetric bilinear groups. In particular, this result can be used to reduce the proof size that $n$ commitments open to a bit-string from $6n\s$ to $(4n+2)\s$.

\subsection{QA Aggregation of Other Equation Types} \label{sec:jutroyaggasym}
Jutla and Roy \cite{C:JutRoy14} show how to aggregate GS proofs of 
two-sided linear equations in symmetric bilinear groups. In the original construction of \cite{C:JutRoy14} soundness is based on a decisional assumption (a weaker variant of the $\lin{2}$ Assumption). Its natural generalization in asymmetric groups (where soundness is based on the SXDH Assumption) only enables to aggregate the proofs of one-sided linear equations. 

In this section, we revisit their construction. We give an alternative, simpler, proof of soundness under a computational assumption which avoids altogether the ``Switching Lemma" of \cite{C:JutRoy14}. Further, we extend it to two-sided equations in the asymmetric setting. For one-sided linear equations we can prove soundness under any kernel assumption and for two-sided linear equations, under any split kernel assumption.\footnote{The results of  \cite{C:JutRoy14} are based on what they call  the ``Switching Lemma". As noted in \cite{EPRINT:MorRafVil15}, it is implicit in the proof of this lemma that the same results can be obtained under computational assumptions.}

Let $A_1,A_2,A_T$ be $\Z_q$-vector spaces compatible with some 
Groth-Sahai equation as detailed in Sect.
\ref{sec:gs}.  Let $\dist_{\Gamma}$ be a witness samplable distribution which outputs $n$ pairs of vectors 
$(\vec{\alpha}_\ell, \vec{\beta}_\ell) \in A_1^{m_y} \times A_2^{m_x}$, $\ell \in [n]$, for some 
$m_x,m_y \in \mathbb{N}$. Given some fixed pairs $(\vec{\alpha}_\ell,
\vec{\beta}_\ell)$, we define, for each $\vecb{\tilde{t}} \in A_{T}^{n}$, the set of equations $\mathcal{S}_{\vecb{\tilde{t}}}$ as:
$$\mathcal{S}_{\vecb{\tilde{t}}}=\left\{ E_{\ell}(\vec{\varx},\vec{\vary})=\tilde{t}_{\ell}: \ell \in [n]\right\},  \qquad 
E_{\ell}(\vec{\varx},\vec{\vary}):=\sum_{j\in[m_y]} f(\alpha_{\ell,j}, \vary_j) + 
\sum_{i\in[m_x]}  f(\varx_i, \beta_{\ell,i}).$$  

%\input{membtwogroups/gs-agg-fig}

We note that, as in \cite{C:JutRoy14}, we only achieve \textit{quasi-adaptive aggregation}, that is, the common reference string is specific to a particular set of equations. More specifically, it depends on the constants $\grkb{\alpha}_{\ell},\grkb{\beta}_{\ell}$ (but not on $\tilde{t}_{\ell}$, which can be chosen by the prover) and it can be used to aggregate the proofs of 
$\mathcal{S}_{\vecb{\tilde{t}}}$, for any~$\vecb{\tilde{t}}$.  

Given the equation types for which we can construct NIZK GS proofs, there always exists (1) $t_{\ell} \in A_1$, such that $\tilde{t}_{\ell}=f(t_{\ell},\mathsf{base}_{2})$ or 
 (2) $\tilde{t}_{\ell} \in A_2,$ such that $\tilde{t}_{\ell}=f(\mathsf{base}_{1},t_{\ell})$, where $\mathsf{base}_{i}=1$ if $\Am_i=\Z_q$, 
 $\mathsf{base}_{1}=\hat{g}$ if $\Am_1=\Gr$ and $\mathsf{base}_{2}=\check{h}$ if   $\Am_2=\Hr$. This is because $\tilde{t}_{\ell}=0_{\T}$ for PPEs, and  $A_{T}=A_{i}$, for some $i \in [2]$, for other types of equations. For simplicity, 
 in the construction we assume that (1) is the case, otherwise 
 change $\iota_2(a_{\ell,i}),  \iota_1(t_{\ell})$ for $\iota_1(a_{\ell,i}),  \iota_2(t_{\ell})$ in the construction below. 

\begin{description}
\item[$\algK_0(1^\lambda)$:]  Return $\Gamma := (q,\Gr,\Hr,\T,e,\hat{g},\check{h}) \leftarrow \ggen_a(1^{\lambda})$.

\item[$\dist_\Gamma$:] $\dist_\Gamma$ is some distribution over $n$ pairs of vectors  $(\grkb{\alpha}_{\ell}$, $\grkb{\beta}_{\ell}) \in A_1^{m_x} \times A_2^{m_y}$.

\item[$\algK_1(\Gamma, \mathcal{S}_{\vecb{\tilde{t}}})$:]
Let $\matr{A}=(a_{i,j}) \gets \dist_{n,k}$. Define 
$$\crs:=\left(\crs_{\GS}, \left\{\sum_{\ell\in[n]}\iota_1(a_{\ell,i} \grkb{\alpha}_\ell), \sum_{\ell\in[n]} \iota_2(a_{\ell,i} \grkb{\beta}_\ell), \big\{\iota_{2}(a_{\ell,i}): \ell \in [n]\big\}: i \in [k]\right\}\right)$$
\item[$\algP(\Gamma, \mathcal{S}_{\vecb{\tilde{t}}},\vecb{x},\vecb{y})$:] 
Given a solution $\vec{\varx}=\vecb{x}$,  $\vec{\vary}=\vecb{y}$ to $\mathcal{S}_{\vecb{\tilde{t}}}$, the prover proceeds as follows:
\begin{itemize}
\item Commit to all $x_j \in A_1$ as $\hvecb{c}_j\gets\mathsf{Comm}_{\GS}(x_j)$, and to all 
$\vary_j\in\Am_2$ as $\cvecb{d}_j\gets\mathsf{Comm}_{\GS}(y_j)$.
%for $\vecb{r}_i \gets \Z_q^{\nu_1}$,  $\vecb{s}_j\gets\Z_q^{\nu_2}$ in the appropriate randomness space. 

\item For each $i \in [k]$, run the GS prover for the equation $\sum_{\ell\in[n]} a_{\ell,i} E_{\ell}(\vec{\varx},\vec{\vary})= \sum_{\ell \in [n]} f(t_{\ell}, a_{\ell,i})$ to obtain the proof, which is a pair  $(\hmatr{\Theta}_i,\cmatr{\Pi}_i)$.
%\footnote{More specifically, 
%$\cmatr{\Pi}_{i} := (\sum_{\ell\in[n]}a_{\ell,i} \iota_2(\grkb{\beta}_\ell^\top))\matr{R}^\top$ and $\hmatr{\Theta}_{i}:=
%(\sum_{\ell\in[n]} a_{\ell,i} \iota_1(\grkb{\alpha}_\ell^\top))\matr{S}^\top$.
%} 
\end{itemize}
Output 
$(\{\hvecb{c}_j : j \in [m_x]\}, \{\cvecb{d}_j: j \in [m_y]\}, \{(\cmatr{\Pi}_{i},\hmatr{\Theta}_{i}) : i \in [k]\})$.
\item[$\algV(\crs,\mathcal{S}_{\vecb{\tilde{t}}},\{\hvecb{c}_j\}_{j\in[m_x]}, \{\cvecb{d}_j\}_{j\in[m_y]}, \{\hmatr{\Theta}_{i},\cmatr{\Pi}_{i}\}_{i \in [k]})$:] For each $i \in [k]$, run the GS verifier for equation
$$\sum_{\ell\in[n]} a_{\ell,i} E_{\ell}(\vec{\varx},\vec{\vary})= \sum_{\ell \in [n]} f(t_{\ell}, a_{\ell,i}).$$
%that is, check if for all $i\in[k]$:
%\begin{eqnarray*}
%\sum_{\substack{\ell\in[n]\\j\in[m_y]}} a_{\ell,i} \iota_1(\alpha_{\ell,j})\cvecb{d}_j^\top +
%\sum_{\substack{\ell\in[n]\\j\in[m_x]}}\hvecb{c}_j a_{\ell,i} \iota_2(\beta_{\ell, j})^\top  &=&
%\sum_{\ell \in [n]} \iota_1(t_\ell) \iota_2(a_{\ell,i})^\top + 
%\hmatr{\Theta}_{i}\cmatr{V}^\top + \hmatr{U}\cmatr{\Pi}_{i}^\top.
%\end{eqnarray*}
\end{description}  

\begin{theorem}
The above protocol is a QA-NIZK proof system for two-sided linear equations.
\end{theorem}  
\begin{proof} {\underline{Completeness.}}  Observe that
\begin{equation}
\sum_{\ell\in[n]} a_{\ell,i} E_{\ell}(\vec{\varx},\vec{\vary})= \sum_{j\in[m_y]} f( a_{\ell,i} \alpha_{\ell,j}, \vary_j) + 
\sum_{j\in[m_x]}  f(\varx_j, a_{\ell,i} \beta_{\ell,j}).
\end{equation}  Completeness follows 
from the observation that to efficiently compute the proof, the GS Prover \cite{ManualBib_SIAMJC:GroSah12} only needs, apart from a satisfying assignment to the equation, the randomness used in the commitments plus a way to compute the inclusion map of all involved constants, in this case $\iota_1(a_{\ell,i} \alpha_{\ell,j})$,
$\iota_2(a_{\ell,i} \beta_{\ell,j})$ and the latter is part of the CRS.
 
 \noindent{\underline{Soundness.}} We change to a game $\mathsf{Game}_{1}$ where we know the discrete logarithm of the GS commitment key, as well as the discrete logarithms of $(\grkb{\alpha}_{\ell},\grkb{\beta}_{\ell})$, $\ell \in [n]$. This is possible because they are both chosen from a witness samplable distribution.

We now prove that an adversary against the soundness in $\mathsf{Game}_{1}$ can be used to construct an adversary $\advB$ against the 
$\dist_{n,k}\mbox{-}\skermdh$ Assumption, where 
$\dist_{n,k}$ is the matrix distribution used in the CRS generation. 

$\advB$ receives a challenge $(\hmatr{A},\cmatr{A})\in\Gr^{n\times k}\times\Hr^{n\times k}$. Given all the discrete logarithms that $\advB$ knows, it can compute a properly distributed CRS even without knowledge of the discrete logarithm of $\hmatr{A}$. The  soundness adversary outputs commitments $\{\hvecb{c}_j\}_{j\in[m_x]},\{\cvecb{d}_{j}\}_{j\in[m_y]}$ together with proofs $\{\hmatr{\Theta}_{i},\cmatr{\Pi}_{i}\}_{i \in [k]}$, which are accepted by the verifier. 
 
Let $\vecb{x}$ (resp. $\hat{\vecb{x}}$) be the vector of openings of $\{\hvecb{c}_j\}_{j\in[m_x]}$ in $A_1$ (resp. in the group $\Gr$)  and $\vecb{y}$  (resp. $\check{\vecb{y}}$) the vector of openings of $\{\cvecb{d}_j\}_{j\in[m_y]}$ in $A_2$ (resp. in the group $\Hr$). If $A_1=\Gr$  (resp. $A_2=\Hr$) then $\vecb{x}=\hat{\vecb{x}}$  (resp. $\vecb{y}=\check{\vecb{y}}$). The vectors $\hat{\vecb{x}}$ and $\check{\vecb{y}}$ are efficiently computable by $\advB$ who knows the discrete logarithm of the commitment keys. We claim that the pair  $(\hgrkb{\rho},\cgrkb{\sigma}) \in \Gr^{n} \times \Hr^n$, 
$\hgrkb{\rho}:=(\grkb{\beta}_1^\top\hvecb{x} -\hat{t}_{1},\ldots,
\grkb{\beta}_{n}^\top\hvecb{x} -\hat{t}_{n}),  
\cgrkb{\sigma}:=(\grkb{\alpha}_1^\top\cvecb{y},\ldots,
\grkb{\alpha}_{n}^\top\cvecb{y})$, solves the  $\dist_{n,k}\mbox{-}\skermdh$ challenge. 

First, observe that if the adversary is successful in breaking the soundness property, then $\grkb{\rho} \neq \grkb{\sigma}$. Indeed, 
if this is the case there is some index $\ell \in [n]$ such that 
$E_{\ell}(\vecb{x},\vecb{y}) \neq \tilde{t}_{\ell}$, which means that 
$\sum_{j\in[m_y]} f(\alpha_{\ell,j}, \vary_j) \neq 
\sum_{j\in[m_x]}  f(\varx_j, \beta_{\ell,j}) - f(t_{\ell},\mathsf{base}_{2})$.  
If we take discrete logarithms in each side of the equation, this inequality is exactly equivalent to
$\grkb{\rho} \neq \grkb{\sigma}$.


Further, because GS proofs have perfect soundness, $\vecb{x}$ and $\vecb{y}$ satisfy 
the equation $\sum_{\ell\in[n]} a_{\ell,i} E_{\ell}(\vec{\varx},\vec{\vary})= \sum_{\ell \in [n]} f(t_{\ell}, a_{\ell,i})$, for all $i \in [k]$,
Thus, for all $i\in[k]$, 
\begin{equation}
\sum_{\ell\in[n]} \check{a}_{\ell,i} \left(\grkb{\beta}_\ell^\top\hvecb{x} - \hat{t}_{\ell} \right)  = \sum_{\ell \in [n]} \hat{a}_{\ell,i}
\left(\grkb{\alpha}^\top_\ell\cvecb{y}\right),
\label{eq:gs-ker}
\end{equation}
which implies that $\hgrkb{\rho}\cmatr{A}=\cgrkb{\sigma}\hmatr{A}$.

\noindent{\underline{Zero-Knowledge.}}  The same simulator of GS proofs can be used. Specifically
the simulated proof corresponds to $k$ simulated GS proofs.
\end{proof}

\subsubsection{One-Sided Equations.} In the case when $\grkb{\alpha}_{\ell}=\matr{0}$ and $\tilde{t}_{\ell}=f(t_{\ell},\mathsf{base}_{2})$ for some $t_{\ell} \in A_1$, for all $\ell\in[n]$, proofs can be aggregated under a standard Kernel Assumption (and thus, in asymmetric bilinear groups we can choose $k=1$). Indeed, 
in this case, in the soundness proof, the adversary $\advB$ receives $\cmatr{A}\in\Hr^{n\times k}$, an instance of the $\dist_{n,k}-\kermdh_\Hr$ problem. The adversary $\advB$ outputs $\hgrkb{\rho}:=(\grkb{\beta}_1^\top\hvecb{x} -\hat{t}_{1},\ldots,
\grkb{\beta}_{n}^\top\hvecb{x} -\hat{t}_{n}) $ as a solution to the challenge. To see why this works, note that, when $\grkb{\alpha}_{\ell}=\matr{0}$ for all $\ell\in[n]$, equation (\ref{eq:gs-ker}) reads $\sum_{\ell\in[n]} \check{a}_{\ell,i} \left(\grkb{\beta}_\ell^\top \hvecb{x} - \hvecb{t}_{\ell} \right)  = \matr{0}_\T$ and thus $\hgrkb{\rho}\cmatr{A}=\matr{0}_\T$.  The case when $\grkb{\beta}_\ell=\matr{0}$ and $\tilde{t}_{\ell}=f(\mathsf{base}_{1},t_{\ell})$ for some $t_{\ell} \in A_2$, for all $\ell\in[n]$, is analogous. 

\subsubsection{Public Parameters.} The size of the CRS of the construction above depends on the number of elements needed to represent $\hmatr{A}$. In this sense, it is interesting to sample $\hmatr{A}$ from some family of matrix assumptions with good representation size. As we assume that $n>k$, it is interesting to instantiate this scheme with the \textit{Circulant Matrix Distribution} of \cite{EPRINT:MorRafVil15}, which has a representation size of $n$ --- independent of $k$. 


%\paragraph{Symmetric bilinear groups.}  The size of the CRS of the construction below depends on the number of elements needed to represent $\hmatr{A}$, while in \cite{C:JutRoy14}, $\hmatr{A}$ is a uniform matrix. This generalization allows to have shorter public parameters than the  \cite{C:JutRoy14} in the symmetric case if  $\hmatr{A}$  is sampled from a family of matrix assumptions with good representation size. In particular, if $\hmatr{A}$ is sampled from the circulant matrix dis

 
%In summary, this means that with the techniques of \cite{C:JutRoy14} and our ``Split'' type of assumptions, we can aggregate two-sided equations of all types under the same restrictions as \cite{C:JutRoy14} and the proof size is $k$ times the proof-size of a single equation. 


