In this section we discuss two different ways to aggregate GS equations. The first is a direct application of the proof of equal commitment opening and is only valid for two-sided linear equations in $\Z_q$, the second is an extension of the results of Jutla and Roy for all other types of linear equations.

\subsection{Aggregating Two-Sided Linear Equations in $\Z_q$}
\label{sec:ts-zq}
We note that proving that $n$ pairs of GS commitments open (pairwise) to the same elements in $\Z_q$ is simply a special case of 
the proof of equal commitment opening in Section~\ref{sec:aggcommit}. Indeed, the concatenation of $n$ GS commitments is just a commitment to a vector of scalars. In particular,  given $\mathsf{crs}_\GS=({gk},[\vecb{u}_1]_1,[\vecb{u}_2]_1,[\vecb{v}_1]_2,[\vecb{v}_2]_2)$,  it is easy to see that  $n$ commitments to $x_i \in \Z_q$, which are of the form:
 $[\vecb{c}_i]_1= x_i [\vecb{u}_1]_1 +r_i [\vecb{u}_2]_1$ for some $r_i \in \Z_q$ (recall that $\iota_1(x_i)= x_i [\vecb{u}_1]_1$), can be written as  
 $$\begin{pmatrix} [\vecb{c}_1]_1 \\ \vdots \\ [\vecb{c}_n]_1 \end{pmatrix}=   \begin{pmatrix} [\vecb{u}_1]_1 & \ldots & [\vecb{0}]_1\\ \vdots & \ddots & \vdots \\   [\vecb{0}]_1 & \ldots & [\vecb{u}_1]_1  \end{pmatrix} \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}
 + \begin{pmatrix} [\vecb{u}_2]_1 & \ldots & [\vecb{0}]_1\\ \vdots & \ddots & \vdots \\  [\vecb{0}]_1 & \ldots & [\vecb{u}_2]_1  \end{pmatrix}
 \begin{pmatrix} r_1 \\ \vdots \\ r_n \end{pmatrix},
 $$
and similarly the concatenation of $n$ commitments $[\vecb{d}_i]_2$, $i \in [\ell]$ can be written as $[\matr{V}^1]_2\vecb{y}+[\matr{V}^2]_2 \vecb{s}$, where $[\matr{V}^i]_2$ is the block-wise concatenation of $n$ copies of $[\vecb{v}_i]_2$.  

In particular, proving that $n$ GS commitments open to the same value can be also seen as the aggregation of the proof  of $n$ GS equations of the form $\varx_\ell -\vary_\ell=0$. The aggregation of any other set of two-sided linear equations in $\Z_q$
easily reduces to this case using the homomorphic properties of GS commitments. Indeed, given $n$ equations of the form:
 $$  \grkb{\alpha}_{\ell}^{\top} \vvary+ \vvarx^{\top}\boldsymbol \beta_{\ell} =t_\ell, \ \ell \in [n],$$
 and the commitments to a satisfying assignment (where the commitments to every coordinate of $\vvarx$ (resp. $\vvary$) are in $\Gr$ (resp. $\Hr$), it is easy to derive a commitment to $\vvarx^{\top} \grkb{\beta}_{\ell}- t_{\ell}$ in $\Gr$ and a commitment to  $\grkb{\alpha}_{\ell}^{\top} \vvary$ in $\Hr$ for all $\ell \in [n]$. Obviously, the equations are satisfied if for each $\ell$, these commitments open to the same value. 

%We insist that two-sided linear equations in $\Z_q$ are essential to prove quadratic statements in asymmetric bilinear groups. In particular, this result can be used to reduce the proof size that $n$ commitments open to a bit-string from $6n\s$ to $(4n+2)\s$.

\subsection{QA Aggregation of Other Equation Types} \label{sec:jutroyaggasym}
Jutla and Roy \cite{C:JutRoy14} show how to aggregate GS proofs of 
two-sided linear equations in symmetric bilinear groups. In the original construction of Jutla and Roy soundness is based on a decisional assumption (a weaker variant of the $\lin{2}$ assumption). Its natural generalization in asymmetric groups (where soundness is based on the SXDH assumption) only enables to aggregate the proofs of one-sided linear equations. 

In this section, we revisit their construction. We give an alternative, simpler, proof of soundness under a computational assumption which avoids altogether the ``switching lemma" of Jutla and Roy. Further, we extend it to two-sided equations in the asymmetric setting. For one-sided linear equations we can prove soundness under any kernel assumption and for two-sided linear equations, under any split kernel assumption.\footnote{The results of Jutla and Roy are based on what they call  the ``switching lemma". As noted Morillo et al.~\cite{EPRINT:MorRafVil15}, it is implicit in the proof of this lemma that the same results can be obtained under computational assumptions.}

Let $A_1,A_2,A_T$ be $\Z_q$-vector spaces compatible with some 
Groth-Sahai equation as detailed in Section
\ref{sec:gs-proofs}.  Let $\dist_{{gk}}$ be a witness samplable distribution which outputs $n$ pairs of vectors
$(\grkb{\alpha}_\ell, \grkb{\beta}_\ell) \in A_1^{m_y} \times A_2^{m_x}$, $\ell \in [n]$, for some 
$m_x,m_y \in \mathbb{N}$. Given some fixed pairs $(\grkb{\alpha}_\ell,
\grkb{\beta}_\ell)$, we define, for each $\vecb{\tilde{t}} \in A_{T}^{n}$, the set of equations $\mathcal{S}_{\vecb{\tilde{t}}}$ as:
$$\mathcal{S}_{\vecb{\tilde{t}}}=\left\{ E_{\ell}(\vvarx,\vvary)=\tilde{t}_{\ell}: \ell \in [n]\right\},  \quad 
E_{\ell}(\vvarx,\vvary):=\sum_{j\in[m_y]} f(\alpha_{\ell,j}, \vary_j) + 
\sum_{i\in[m_x]}  f(\varx_i, \beta_{\ell,i}).$$  

We note that, as in Jutla and Roy's work, we only achieve \textit{quasi-adaptive aggregation}, that is, the common reference string is specific to a particular set of equations. More specifically, it depends on the constants $\grkb{\alpha}_{\ell},\grkb{\beta}_{\ell}$ (but not on $\tilde{t}_{\ell}$, which can be chosen by the prover) and it can be used to aggregate the proofs of 
$\mathcal{S}_{\vecb{\tilde{t}}}$, for any~$\vecb{\tilde{t}}$.  

Given the equation types for which we can construct NIZK GS proofs (and not only NIWI proofs), there always exists (1) $t_{\ell} \in A_1$, such that $\tilde{t}_{\ell}=f(t_{\ell},\mathsf{base}_{2})$ or 
 (2) ${t}_{\ell} \in A_2,$ such that $\tilde{t}_{\ell}=f(\mathsf{base}_{1},t_{\ell})$, where $\mathsf{base}_{i}=1$ if $\Am_i=\Z_q$ and $\mathsf{base}_i=\mathcal{P}_i$ if $A_i=\GG_i$, $i \in \{1,2\}$ \cite{SIAMJC:GroSah12}. For simplicity, 
 in the construction we assume that (1) is the case, otherwise 
 change $\iota_2(a_{\ell,i}),  \iota_1(t_{\ell})$ for $\iota_1(a_{\ell,i}),  \iota_2(t_{\ell})$ in the construction below. 

\begin{description}
%\item[$\algK_0(1^\lambda)$:]  Return ${gk} := (q,\Gr,\Hr,\GG_T,e,\mathcal{P}_1,\mathcal{P}_2) \leftarrow \ggen_a(1^{\lambda})$.

%\item[$\dist_{gk}$:] $\dist_{gk}$ is some distribution over $n$ pairs of vectors  $(\grkb{\alpha}_{\ell}$, $\grkb{\beta}_{\ell}) \in A_1^{m_x} \times A_2^{m_y}$.

\item[$\algK({gk}, \mathcal{S}_{\vecb{\tilde{t}}})$:]
Let $\matr{A}=(a_{i,j}) \gets \dist_{n,k}$. Define 
$$\crs:=\left(\crs_{\GS}, \left\{\sum_{\ell\in[n]}\iota_1(a_{\ell,i} \grkb{\alpha}_\ell), \sum_{\ell\in[n]} \iota_2(a_{\ell,i} \grkb{\beta}_\ell), \big\{\iota_{2}(a_{\ell,i}): \ell \in [n]\big\}: i \in [k]\right\}\right)$$
\item[$\algP({gk}, \mathcal{S}_{\vecb{\tilde{t}}},\vecb{x},\vecb{y})$:] 
Given a solution $\vvarx=\vecb{x}$,  $\vvary=\vecb{y}$ to $\mathcal{S}_{\vecb{\tilde{t}}}$, the prover proceeds as follows:
\begin{itemize}
\item Commit to all $x_j \in A_1$ as $[\vecb{c}_j]_1\gets\mathsf{GS.Com}(x_j)$, and to all 
$y_j\in\Am_2$ as $[\vecb{d}_j]_2\gets\mathsf{GS.Com}(y_j)$.

\item For each $i \in [k]$, run the GS prover for the equation $\sum_{\ell\in[n]} a_{\ell,i} E_{\ell}(\vvarx,\vvary)= \sum_{\ell \in [n]} f(t_{\ell}, a_{\ell,i})$ to obtain the proof, which is a pair  $([\matr{\Theta}_i]_1,[\matr{\Pi}_i]_2)$.
\end{itemize}
Output 
$(\{[\vecb{c}_j]_1 : j \in [m_x]\}, \{[\vecb{d}_j]_2: j \in [m_y]\}, \{([\matr{\Pi}_{i}]_2,[\matr{\Theta}_{i}]_1) : i \in [k]\})$.
\item[$\algV(\crs,\mathcal{S}_{\vecb{\tilde{t}}},{\{[\vecb{c}_j]_1 : j\in[m_x]\},\{[\vecb{d}_j]_2:j\in[m_y]\}, \{[\matr{\Theta}_{i}]_1,[\matr{\Pi}_{i}]_2:i \in [k]\}})$:] For each $i \in [k]$, run the GS verifier for equation
$$\sum_{\ell\in[n]} a_{\ell,i} E_{\ell}(\vvarx,\vvary)= \sum_{\ell \in [n]} f(t_{\ell}, a_{\ell,i}).$$
\end{description}  

\begin{theorem}
The above protocol is a QA-NIZK proof system for two-sided linear equations.
\end{theorem}  
\begin{proof}\ 
(Completeness.)  Observe that for each $i\in[k]$
\begin{equation}
\sum_{\ell\in[n]} a_{\ell,i} E_{\ell}(\vvarx,\vvary)= \sum_{\ell\in[n]} a_{\ell,i}\tilde{t}_\ell=\sum_{\ell\in[n]}f(t_\ell,a_{\ell,i}).
\end{equation}  Completeness follows 
from the observation that to efficiently compute the proof, the GS prover only needs, apart from a satisfying assignment to the equation, the randomness used in the commitments plus a way to compute the inclusion map of all involved constants, in this case $\iota_1(a_{\ell,i} \alpha_{\ell,j})$,
$\iota_2(a_{\ell,i} \beta_{\ell,j})$, and the latter is part of the CRS.
 
\noindent{(Soundness.)} We change to a game $\mathsf{Game}_{1}$ where we know the discrete logarithm of the GS commitment key, as well as the discrete logarithms of $(\grkb{\alpha}_{\ell},\grkb{\beta}_{\ell})$, $\ell \in [n]$. This is possible because they are both chosen from a witness samplable distribution.

We now prove that an adversary against the soundness in $\mathsf{Game}_{1}$ can be used to construct an adversary $\advB$ against the 
$\dist_{n,k}\mbox{-}\skermdh$ assumption, where 
$\dist_{n,k}$ is the matrix distribution used in the CRS generation. 

$\advB$ receives a challenge $([\matr{A}]_1,[\matr{A}]_2)\in\Gr^{n\times k}\times\Hr^{n\times k}$. Given all the discrete logarithms that $\advB$ knows, it can compute a properly distributed CRS even without knowledge of the discrete logarithm of $[\matr{A}]_1$. The  soundness adversary outputs commitments $\{[\vecb{c}_j]_1:j\in[m_x]\},\{[\vecb{d}_{j}]_2:j\in[m_y]\}$ together with proofs $\{[\matr{\Theta}_{i}]_1,[\matr{\Pi}_{i}]_2:i \in [k]\}$, which are accepted by the verifier.

The adversary $\advB$ can use the discrete logarithm of the commitment keys to compute openings of $\{[\vecb{c}_j]_1:j\in[m_x]\},\{[\vecb{d}_j]_2:j\in[m_y]\}$ (or the corresponding translation to $\GG_s$ when $A_s=\mathbb{Z}_q, s\in\{1,2\}$). Let $[\vecb{x}]_1$ and $[\vecb{y}]_2$ the vectors of these commitments.
We claim that the pair  $([\grkb{\rho}]_1,[\grkb{\sigma}]_2) \in \Gr^{n} \times \Hr^n$, 
$[\grkb{\rho}]_1:=(\grkb{\beta}_1^\top[\vecb{x}]_1 -[t_1]_1,\ldots,
\grkb{\beta}_{n}^\top[\vecb{x}]_1 -[t_{n}]_1),  
[\grkb{\sigma}]_2:=(\grkb{\alpha}_1^\top[\vecb{y}]_2,\ldots,
\grkb{\alpha}_{n}^\top[\vecb{y}]_2)$, solves the  $\dist_{n,k}\mbox{-}\skermdh$ challenge and can be efficiently computed by $\advB$. 

First, observe that if the adversary is successful in breaking the soundness property, then $\grkb{\rho} \neq \grkb{\sigma}$. Indeed, 
if this is the case there is some index $\ell \in [n]$ such that 
$E_{\ell}(\vecb{x},\vecb{y}) \neq \tilde{t}_{\ell}$, which means that 
$\sum_{j\in[m_y]} f(\alpha_{\ell,j}, \vary_j) \neq 
\sum_{j\in[m_x]}  f(\varx_j, \beta_{\ell,j}) - f(t_{\ell},\mathsf{base}_{2})$.  
If we take discrete logarithms in each side of the equation, this inequality is exactly equivalent to
$\grkb{\rho} \neq \grkb{\sigma}$.


Further, because GS proofs have perfect soundness, $\vecb{x}$ and $\vecb{y}$ satisfy 
the equation $\sum_{\ell\in[n]} a_{\ell,i} E_{\ell}(\vvarx,\vvary)= \sum_{\ell \in [n]} f(t_{\ell}, a_{\ell,i})$, for all $i \in [k]$.
Thus, for all $i\in[k]$, 
\begin{equation}
\sum_{\ell\in[n]} [{a}_{\ell,i}]_2 \left(\grkb{\beta}_\ell^\top[\vecb{x}]_1 - [{t}_{\ell}]_1 \right)  = \sum_{\ell \in [n]} [{a}_{\ell,i}]_1
\left(\grkb{\alpha}^\top_\ell[\vecb{y}]_2\right),
\label{eq:gs-ker}
\end{equation}
which implies that $[\grkb{\rho}]_1[\matr{A}]_2=[\grkb{\sigma}]_2[\matr{A}]_1$.

\noindent{(Zero-Knowledge.)}  The same simulator of GS proofs can be used. Specifically
the simulated proof corresponds to $k$ simulated GS proofs.
\end{proof}

\subsubsection{One-Sided Equations.} In the case when $\grkb{\alpha}_{\ell}=\matr{0}$ and $\tilde{t}_{\ell}=f(t_{\ell},\mathsf{base}_{2})$ for some $t_{\ell} \in A_1$, for all $\ell\in[n]$, proofs can be aggregated under a standard kernel assumption (and thus, in asymmetric bilinear groups we can choose $k=1$). Indeed, 
in this case, in the soundness proof, the adversary $\advB$ receives $[\matr{A}]_2\in\Hr^{n\times k}$, an instance of the $\dist_{n,k}\mbox{-}\kermdh_{\Hr}$ problem. The adversary $\advB$ outputs $[\grkb{\rho}]_1:=(\grkb{\beta}_1^\top[\vecb{x}]_1 -[{t}_{1}]_1,\ldots,
\grkb{\beta}_{n}^\top[\vecb{x}]_1 -[{t}_{n}]_1) $ as a solution to the challenge. To see why this works, note that, when $\grkb{\alpha}_{\ell}=\matr{0}$ for all $\ell\in[n]$, equation (\ref{eq:gs-ker}) reads $\sum_{\ell\in[n]} [{a}_{\ell,i}]_2 \left(\grkb{\beta}_\ell^\top [\vecb{x}]_1 - [{t}_{\ell}]_1 \right)  = [{0}]_T$ and thus $[\grkb{\rho}]_1[\matr{A}]_2=[\matr{0}]_T$.  The case when $\grkb{\beta}_\ell=\matr{0}$ and $\tilde{t}_{\ell}=f(\mathsf{base}_{1},t_{\ell})$ for some $t_{\ell} \in A_2$, for all $\ell\in[n]$, is analogous. 

\subsubsection{Public Parameters.} The size of the CRS of the construction above depends on the number of elements needed to represent $[\matr{A}]_2$. In this sense, it is interesting to sample $[\matr{A}]_2$ from some family of matrix assumptions with good representation size. As we assume that $n>k$, it is interesting to instantiate this scheme with the \textit{circulant matrix distribution} of Morillo et al.~\cite{EPRINT:MorRafVil15}, which has a representation size of $n$ --- independent of $k$. 


%\paragraph{Symmetric bilinear groups.}  The size of the CRS of the construction below depends on the number of elements needed to represent $\hmatr{A}$, while in \cite{C:JutRoy14}, $\hmatr{A}$ is a uniform matrix. This generalization allows to have shorter public parameters than the  \cite{C:JutRoy14} in the symmetric case if  $\hmatr{A}$  is sampled from a family of matrix assumptions with good representation size. In particular, if $\hmatr{A}$ is sampled from the circulant matrix dis

 
%In summary, this means that with the techniques of \cite{C:JutRoy14} and our ``Split'' type of assumptions, we can aggregate two-sided equations of all types under the same restrictions as \cite{C:JutRoy14} and the proof size is $k$ times the proof size of a single equation. 


